{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_dir=\"database/1103output/test.json\"\n",
    "\n",
    "with open(test_json_dir,\"r\") as f:\n",
    "    test_list=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ref_ids=[i[\"ref_id\"] for i in test_list if \"ref_id\" in i]\n",
    "len(test_ref_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dirs=glob.glob(\"database/1103output/context_list*.json\")\n",
    "\n",
    "context_list_list=[]\n",
    "for path in sorted(context_dirs):\n",
    "    with open(path,\"r\") as f:\n",
    "        l=json.load(f)\n",
    "    context_list_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduction target\n",
    "\n",
    "context_ids=[i[\"context_id\"] for i in test_list if \"context_id\" in i]\n",
    "context_ids=list(set(context_ids))\n",
    "\n",
    "extracted_context_sets=[]\n",
    "\n",
    "for context_list in context_list_list:\n",
    "    extracted_items = [context_list[i] for i in context_ids if i < len(context_list)]\n",
    "    extracted_context_sets.extend(extracted_items)\n",
    "\n",
    "with open(\"database/1111split/target/intro_eng.json\",\"w\") as f:\n",
    "    json.dump(extracted_context_sets[:len(context_ids)],f,indent=4)\n",
    "\n",
    "with open(\"database/1111split/target/intro_esp_ger_ita.json\",\"w\") as f:\n",
    "    json.dump(extracted_context_sets[len(context_ids):],f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13789"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant 1 / intro\n",
    "import random\n",
    "extracted_context_sets=[]\n",
    "for context_list in context_list_list:\n",
    "    excluded_items = [item for idx, item in enumerate(context_list) if idx not in context_ids]\n",
    "    extracted_context_sets.extend(excluded_items)\n",
    "\n",
    "with open(f\"database//1111split/irrelevant1/intro_eng.json\",\"w\") as f:\n",
    "    json.dump(extracted_context_sets[:len(excluded_items)],f,indent=4)\n",
    "with open(f\"database//1111split/irrelevant1/intro_esp_ger_ita.json\",\"w\") as f:\n",
    "    json.dump(extracted_context_sets[len(excluded_items):],f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abst concn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    pattern = r'\\.(?=[A-Za-z])'\n",
    "    text= re.sub(pattern, '. ', text)\n",
    "    #連続したピリオドを削除\n",
    "    text=re.sub(r'\\.{2,}', '.', text)\n",
    "    return text\n",
    "\n",
    "def unify_text(text):\n",
    "    text = text.replace(\"。\", \"\\n\")\n",
    "    text = text.replace(\".\", \"\\n\")\n",
    "    text = text.replace(\"．\", \"\\n\")\n",
    "    text = text.replace(\"？\", \"\\n\")\n",
    "    text = text.replace(\"?\", \"\\n\")\n",
    "    text = text.replace(\"！\", \"\\n\")\n",
    "    text = text.replace(\"!\", \"\\n\")\n",
    "    text = text.replace(\"；\", \"\\n\")\n",
    "    text = text.replace(\";\", \"\\n\")\n",
    "    text = text.replace(\"：\", \"\\n\")\n",
    "    text = text.replace(\":\", \"\\n\")\n",
    "    text = text.replace(\"　\", \" \")\n",
    "\n",
    "    return text\n",
    "\n",
    "def split_text(text, chunk_size_limit=2000):\n",
    "    text = unify_text(text)\n",
    "\n",
    "    text_list = text.split(\"\\n\")\n",
    "\n",
    "    chunk_text_list = []\n",
    "    temp_text = \"\"\n",
    "    for t in text_list:\n",
    "        t = t.strip()\n",
    "        temp_text += t+\".\"\n",
    "        if len(temp_text) < chunk_size_limit:\n",
    "            continue\n",
    "        else:\n",
    "            chunk_text_list.append(temp_text)\n",
    "            temp_text = \"\"\n",
    "\n",
    "    chunk_text_list.append(clean_text(temp_text))\n",
    "\n",
    "    chunk_text_list = [t for t in chunk_text_list if len(t) > 10]\n",
    "    return chunk_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abst\n",
    "with open(\"database/1103output/ref_list.json\",\"r\") as f:\n",
    "    ref_list=json.load(f)\n",
    "\n",
    "with open(\"database/1103output/context_to_ref_id.json\",\"r\") as f:\n",
    "    context_to_ref_id=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "used_ref_list=list(set([r[\"ref_id\"] for r in context_to_ref_id]))\n",
    "abst_list=[r[\"abstract\"] for r in ref_list if r[\"ref_id\"] in used_ref_list]\n",
    "concn_list=[r[\"main\"][2][\"text\"] for r in ref_list if r[\"ref_id\"] in used_ref_list if len(r[\"main\"])>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64666/64666 [00:22<00:00, 2816.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "target_chunk_abst_list=[]\n",
    "irrelevant_chunk_abst_list=[]\n",
    "target_chunk_concn_list=[]\n",
    "irrelevant_chunk_concn_list=[]\n",
    "\n",
    "\n",
    "for r in tqdm(ref_list):\n",
    "    abst=r[\"abstract\"]\n",
    "\n",
    "    try:\n",
    "        concn=r[\"main\"][2][\"text\"]\n",
    "    except:\n",
    "        concn=\"\"\n",
    "\n",
    "    chunk_abst=[]\n",
    "    chunk_concn=split_text(concn)\n",
    "    chunk_abst=split_text(abst)\n",
    "\n",
    "    if r[\"ref_id\"] in test_ref_ids:\n",
    "        target_chunk_abst_list.extend(chunk_abst)\n",
    "        target_chunk_concn_list.extend(chunk_concn)\n",
    "    elif r[\"ref_id\"] in used_ref_list:\n",
    "        irrelevant_chunk_abst_list.extend(chunk_abst)\n",
    "        irrelevant_chunk_concn_list.extend(chunk_concn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 950, 13829, 46222)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_chunk_abst_list),len(target_chunk_concn_list),len(irrelevant_chunk_abst_list),len(irrelevant_chunk_concn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"database/1111split/target/abst_eng.json\",\"w\") as f:\n",
    "    json.dump(target_chunk_abst_list,f,indent=4)\n",
    "\n",
    "with open(\"database/1111split/target/concn_eng.json\",\"w\") as f:\n",
    "    json.dump(target_chunk_concn_list,f,indent=4)\n",
    "\n",
    "with open(\"database/1111split/irrelevant1/abst_eng.json\",\"w\") as f:\n",
    "    json.dump(irrelevant_chunk_abst_list,f,indent=4)\n",
    "\n",
    "with open(\"database/1111split/irrelevant1/concn_eng.json\",\"w\") as f:\n",
    "    json.dump(irrelevant_chunk_concn_list,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruct\n",
    "!cp database/1103output/train.json database/1111split/irrelevant1/instruct_eng.json\n",
    "!cp database/1103output/test.json database/1111split/target/instruct_eng.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64666/64666 [00:17<00:00, 3759.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# irrelevant 2 intro\n",
    "\n",
    "irreleavnt2_intro_list=[]\n",
    "for r in tqdm(ref_list):\n",
    "    if r[\"ref_id\"] not in used_ref_list:\n",
    "        intro=r[\"main\"][0][\"text\"]\n",
    "        intro_chunk=split_text(intro)\n",
    "        irreleavnt2_intro_list.extend(intro_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"database/1111split/irrelevant2/intro_eng.json\",\"w\") as f:\n",
    "    json.dump(irreleavnt2_intro_list,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp database/1103output/mmlu_train.json database/1111split/mmlu_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136092"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"database/1111split/irrelevant2/intro_eng.json\",\"r\") as f:\n",
    "    irreleavnt2_intro_list=json.load(f)\n",
    "len(irreleavnt2_intro_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
