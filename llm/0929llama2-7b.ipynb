{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/ft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#ライブラリの自動リロード\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#!pip install rouge-score\n",
    "#!pip install peft\n",
    "#!pip install bitsandbytes\n",
    "#!pip install accelerate\n",
    "#!pip install scipy\n",
    "#!huggingface-cli login --token=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\n",
    "from scoring import eval_model\n",
    "from transformers import pipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"zero\"\n",
    "mode=\"qlora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-29 17:53:58,501] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/ft/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#deepspeed\n",
    "import torch\n",
    "import deepspeed\n",
    "from transformers.deepspeed import HfDeepSpeedConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import json\n",
    "\n",
    "if mode==\"zero\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\",0))\n",
    "    world_size = int(os.getenv(\"WORLD_SIZE\",1))\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    deepspeed.init_distributed()\n",
    "\n",
    "    # ベースとなるZeRO3 configの読み込み\n",
    "    ds_config_file = \"zero_infer.json\"\n",
    "    with open(ds_config_file) as f:\n",
    "        ds_config = json.load(f)\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    hidden_size = model_config.hidden_size\n",
    "\n",
    "    ds_config[\"train_batch_size\"] = 1 * world_size\n",
    "    ds_config[\"train_micro_batch_size_per_gpu\"] = 1\n",
    "    ds_config[\"reduce_bucket_size\"] = hidden_size*hidden_size\n",
    "    ds_config[\"stage3_prefetch_bucket_size\"] = 0.9 * hidden_size * hidden_size\n",
    "    ds_config[\"stage3_param_persistence_threshold\"] = 10 * hidden_size\n",
    "\n",
    "    dschf = HfDeepSpeedConfig(ds_config)  #zero3を使用するために必要(モデルロード前に実行する必要がある)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\n",
    "    ds_model = ds_engine.module#.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#qloraの場合\n",
    "if mode==\"qlora\":\n",
    "    from peft import PeftModel\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "    #load base model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                quantization_config=bnb_config, \n",
    "                                                device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe= pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_path=\"../database/output/qa_dataset.json\"\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    raw_dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_model(raw_dataset[:50],pipe,\"outputs/0929test/original_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6721.64it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 857.03it/s]\n",
      "Generating train split: 10000 examples [00:00, 88214.41 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7260.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "context_path=\"../database/output/context0926.json\"\n",
    "with open(context_path, 'r') as f:\n",
    "    context_list = json.load(f)\n",
    "\n",
    "context_list=context_list[:10000]\n",
    "\n",
    "train_text_path=\"trainset/temp_train.txt\"\n",
    "with open(train_text_path,\"w\") as f:\n",
    "    for context in context_list:\n",
    "        f.write(context+\"\\n\")\n",
    "\n",
    "if mode==\"qlora\": \n",
    "    train_dataset = load_dataset(\"text\", data_files=train_text_path)\n",
    "    train_dataset=train_dataset.map(lambda samples: tokenizer(samples['text']), batched=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if mode==\"zero\":\n",
    "    from transformers import TextDataset\n",
    "    train_dataset= TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_text_path,\n",
    "        block_size=4096, #文章の長さを揃える,\n",
    "        cache_dir=\"cache/\"+model_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 21:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.088000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=2.1544044189453126, metrics={'train_runtime': 1270.1839, 'train_samples_per_second': 7.873, 'train_steps_per_second': 0.787, 'total_flos': 9.101298772721664e+16, 'train_loss': 2.1544044189453126, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_device_train_batch_size=10\n",
    "epochs=1\n",
    "\n",
    "train_args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=epochs,  # エポック数\n",
    "        #max_steps=tot_steps,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir='outputs',\n",
    "        #optim=\"\"\n",
    "    )\n",
    "\n",
    "if mode==\"qlora\":\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    peft_config = LoraConfig(\n",
    "            task_type=\"CAUSAL_LM\", inference_mode=False, r=8, lora_alpha=32,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "if mode==\"zero\":\n",
    "        train_args.deepspeed='./zero_infer.json',  # deepspeedのconfigへのpath\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "if mode==\"zero\":\n",
    "      trainer.gradient_checkpointing=True\n",
    "\n",
    "#model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/10000_qlora/tokenizer_config.json',\n",
       " 'model/10000_qlora/special_tokens_map.json',\n",
       " 'model/10000_qlora/tokenizer.model',\n",
       " 'model/10000_qlora/added_tokens.json',\n",
       " 'model/10000_qlora/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_name = f\"model/10000_{mode}\"\n",
    "trainer.model.save_pretrained(peft_name)\n",
    "tokenizer.save_pretrained(peft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/ft/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [14:04<00:00,  4.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "      <th>pred</th>\n",
       "      <th>score</th>\n",
       "      <th>type</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>3</td>\n",
       "      <td>2. A glucose biofuel cell (GBFC) consists of t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>1</td>\n",
       "      <td>1. LmrR has been shown to bind the compounds s...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>1</td>\n",
       "      <td>2. The purpose of the Japanese phase III clini...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>1</td>\n",
       "      <td>1. SmMIT-LAMP is a LAMP (Loop-mediated isother...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>The key components of a LEID device structure ...</td>\n",
       "      <td>1.CFs with high strength and good electrical c...</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>gen</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>2</td>\n",
       "      <td>2. According to the text, PbO was selected as ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>1</td>\n",
       "      <td>2. In concentrated DNA solutions, in addition ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>The concept that a polymer's molecular structu...</td>\n",
       "      <td>A major branch of polymer technology relies o...</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>gen</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>2</td>\n",
       "      <td>2. According to the Codex Alimentarius Commiss...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>You are a great scientist. Anser the following...</td>\n",
       "      <td>2</td>\n",
       "      <td>2. Identifying new therapeutic targets for hor...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>multi</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               problem  \\\n",
       "0    You are a great scientist. Anser the following...   \n",
       "1    You are a great scientist. Anser the following...   \n",
       "2    You are a great scientist. Anser the following...   \n",
       "3    You are a great scientist. Anser the following...   \n",
       "4    You are a great scientist. Anser the following...   \n",
       "..                                                 ...   \n",
       "195  You are a great scientist. Anser the following...   \n",
       "196  You are a great scientist. Anser the following...   \n",
       "197  You are a great scientist. Anser the following...   \n",
       "198  You are a great scientist. Anser the following...   \n",
       "199  You are a great scientist. Anser the following...   \n",
       "\n",
       "                                                answer  \\\n",
       "0                                                    3   \n",
       "1                                                    1   \n",
       "2                                                    1   \n",
       "3                                                    1   \n",
       "4    The key components of a LEID device structure ...   \n",
       "..                                                 ...   \n",
       "195                                                  2   \n",
       "196                                                  1   \n",
       "197  The concept that a polymer's molecular structu...   \n",
       "198                                                  2   \n",
       "199                                                  2   \n",
       "\n",
       "                                                  pred     score   type  \\\n",
       "0    2. A glucose biofuel cell (GBFC) consists of t...  0.000000  multi   \n",
       "1    1. LmrR has been shown to bind the compounds s...  1.000000  multi   \n",
       "2    2. The purpose of the Japanese phase III clini...  0.000000  multi   \n",
       "3    1. SmMIT-LAMP is a LAMP (Loop-mediated isother...  1.000000  multi   \n",
       "4    1.CFs with high strength and good electrical c...  0.357143    gen   \n",
       "..                                                 ...       ...    ...   \n",
       "195  2. According to the text, PbO was selected as ...  1.000000  multi   \n",
       "196  2. In concentrated DNA solutions, in addition ...  0.000000  multi   \n",
       "197   A major branch of polymer technology relies o...  0.057692    gen   \n",
       "198  2. According to the Codex Alimentarius Commiss...  1.000000  multi   \n",
       "199  2. Identifying new therapeutic targets for hor...  1.000000  multi   \n",
       "\n",
       "     context  \n",
       "0      False  \n",
       "1       True  \n",
       "2       True  \n",
       "3      False  \n",
       "4       True  \n",
       "..       ...  \n",
       "195     True  \n",
       "196    False  \n",
       "197    False  \n",
       "198     True  \n",
       "199     True  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_model(raw_dataset[:200],pipe,f\"res/0929test/10000_{mode}_200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
